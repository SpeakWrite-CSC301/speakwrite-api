{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset to and from Hugging Face.\n",
    "\n",
    "This notebook performs both tasks. First, we focus on loading our CSV file and uploading it to Hugging Face as a dataset.\n",
    "\n",
    "Afterwards, we examine how to load the data back into our working directory for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from datasets import load_dataset, Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload CSV dataset to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CSV dataset into an HF Dataset object\n",
    "\n",
    "We can visualize the data tabularly through it's methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efbf153a5ab42dcae55fba2e8309998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path = \"datasets/gpt4_dataset.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files=csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 0:\n",
      "\n",
      "chat_history: Meeting at 3 PM.\n",
      "Entry 1:\n",
      "\n",
      "chat_history: The report needs final review.\n",
      "Entry 2:\n",
      "\n",
      "chat_history: Flight leaves at 8 AM.\n",
      "Entry 3:\n",
      "\n",
      "chat_history: We need to be there at 12 AM.\n",
      "Entry 4:\n",
      "\n",
      "chat_history: The project deadline is next Friday.\n",
      "Entry 5:\n",
      "\n",
      "chat_history: Client call rescheduled to 2 PM.\n",
      "Entry 6:\n",
      "\n",
      "chat_history: Tasks: Submit report, update spreadsheet.\n",
      "Entry 7:\n",
      "\n",
      "chat_history: Discussed budget updates.\n",
      "Entry 8:\n",
      "\n",
      "chat_history: I Had a quick chat with Sarah.\n",
      "Entry 9:\n",
      "\n",
      "chat_history: Finalized event schedule.\n",
      "Entry 10:\n",
      "\n",
      "chat_history: Finished the quarterly report. Sent it to the team for review. Also drafted the email for stakeholders.\n",
      "Entry 11:\n",
      "\n",
      "chat_history: The client presentation is scheduled for next Monday. We still need to finalize the slides and confirm attendance.\n",
      "Entry 12:\n",
      "\n",
      "chat_history: Team meeting covered progress updates. Discussed blockers and next steps. The new roadmap was introduced as well.\n",
      "Entry 13:\n",
      "\n",
      "chat_history: Spoke with Jake about the upcoming release. He mentioned a potential delay due to testing issues. Need to update the schedule accordingly.\n",
      "Entry 14:\n",
      "\n",
      "chat_history: The supplier confirmed shipment for next week. Still waiting for tracking details. The warehouse team is prepared to receive it.\n",
      "Entry 15:\n",
      "\n",
      "chat_history: Reviewed the marketing strategy for Q2. Identified areas for improvement, especially in social media outreach. Proposed a new content plan.\n",
      "Entry 16:\n",
      "\n",
      "chat_history: The sales team exceeded their targets this month. We should highlight their achievements in the next all-hands meeting.\n",
      "Entry 17:\n",
      "\n",
      "chat_history: Talked to HR about the hiring process. They’re reviewing applications now. The first round of interviews will begin next week.\n",
      "Entry 18:\n",
      "\n",
      "chat_history: The product demo went well. Customers were engaged and asked good questions. Some feedback points need to be addressed before launch.\n",
      "Entry 19:\n",
      "\n",
      "chat_history: Had a call with the vendor about contract renewal. They sent over a revised draft for review. We should go through the changes carefully.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset[\"train\"])):\n",
    "    print(f\"Entry {i}:\")\n",
    "    print(f\"chat_history: \" + dataset[\"train\"][\"chat_history\"][i] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split csv dataset into train and temp (80% train, 20% temp)\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split temp into validation and test (50% each → 10% of total dataset each)\n",
    "valid_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Create the final DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"valid\": valid_test_split[\"train\"],\n",
    "    \"test\": valid_test_split[\"test\"]\n",
    "})\n",
    "\n",
    "# Verify the splits\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to HF\n",
    "\n",
    "We first convert the dataframe to an HF Dataset object, which seamlessly uploads to the HF hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination\n",
    "hf_username = \"\"\n",
    "dataset_name = \"text-edit-dataset-gpt4\"\n",
    "\n",
    "# upload to hub\n",
    "dataset.push_to_hub(f\"{hf_username}/{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model from HF\n",
    "\n",
    "This can be for training, or to overwrite the existing dataset that's pushed to HF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from HF Hub\n",
    "dataset = load_dataset(\"your_hf_username/text_editing_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, validation, test\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "split_dataset = {\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"valid_test\": split_dataset[\"test\"]\n",
    "}\n",
    "split_dataset[\"valid_test\"] = split_dataset[\"valid_test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "# write as python dict\n",
    "dataset_dict = {\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"valid\": split_dataset[\"valid_test\"][\"train\"],\n",
    "    \"test\": split_dataset[\"valid_test\"][\"test\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to DatasetDict object (suitable for training)\n",
    "daataset = DatasetDict(dataset_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
