{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be94e6d6-4096-4d1a-aa58-5afd89f33bff",
   "metadata": {},
   "source": [
    "# SpeakWrite Text-Edit LLM Fine-Tuning Notebook\n",
    "\n",
    "Fine-tuning a pretrained LLM to specialize in predetermined text-editing commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef8ea85-d04d-4217-99a3-21c446bf2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# preprocessing\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "# models\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "# fine-tuning\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "\n",
    "# math\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a4484-07d8-49dd-81ef-672105f53ebe",
   "metadata": {},
   "source": [
    "### Load the Dataset\n",
    "\n",
    "We can either load the entire dataset from Hugging Face, or just parse our local CSV file and split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de226234-c521-4577-802c-0e7079ef4364",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# OPTION 1: Load from HF\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# load a dataset from Hugging Face\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataset_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# TODO: insert our HF dataset path\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(dataset_path)\n\u001b[1;32m      7\u001b[0m \u001b[39m# display the dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# the keys should be the splits, like \"train\", \"validation\", etc.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m dataset\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   2130\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   2131\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   2132\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   2133\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   2134\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2135\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   2136\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2137\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2138\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2139\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2140\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2141\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m   2142\u001b[0m     _require_default_config_name\u001b[39m=\u001b[39;49mname \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2143\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   2144\u001b[0m )\n\u001b[1;32m   2146\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/load.py:1849\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1847\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1848\u001b[0m     download_config\u001b[39m.\u001b[39mstorage_options\u001b[39m.\u001b[39mupdate(storage_options)\n\u001b[0;32m-> 1849\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1850\u001b[0m     path,\n\u001b[1;32m   1851\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1852\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1853\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1854\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1855\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1856\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1857\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m   1858\u001b[0m     _require_default_config_name\u001b[39m=\u001b[39;49m_require_default_config_name,\n\u001b[1;32m   1859\u001b[0m     _require_custom_configs\u001b[39m=\u001b[39;49m\u001b[39mbool\u001b[39;49m(config_kwargs),\n\u001b[1;32m   1860\u001b[0m )\n\u001b[1;32m   1861\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m builder_kwargs \u001b[39m=\u001b[39m dataset_module\u001b[39m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/datasets/load.py:1536\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m download_config\u001b[39m.\u001b[39mforce_extract \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1534\u001b[0m download_config\u001b[39m.\u001b[39mforce_download \u001b[39m=\u001b[39m download_mode \u001b[39m==\u001b[39m DownloadMode\u001b[39m.\u001b[39mFORCE_REDOWNLOAD\n\u001b[0;32m-> 1536\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mfilter\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m x: x, path\u001b[39m.\u001b[39;49mreplace(os\u001b[39m.\u001b[39;49msep, \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m)))[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[1;32m   1537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1538\u001b[0m     filename \u001b[39m=\u001b[39m filename \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# OPTION 1: Load from HF\n",
    "\n",
    "# load a dataset from Hugging Face\n",
    "dataset_path = \"\"  # TODO: insert our HF dataset path\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "# display the dataset\n",
    "# the keys should be the splits, like \"train\", \"validation\", etc.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fe09f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2642a49d741f43f2b8528b2ec2da3487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chat_history', 'current_prompt', 'expected_output'],\n",
       "        num_rows: 154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTION 2: Parse CSV\n",
    "\n",
    "# get the csv as an HF DatasetDict object\n",
    "csv_path = \"datasets/gpt4_dataset.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files=csv_path)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a56f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chat_history', 'current_prompt', 'expected_output'],\n",
       "        num_rows: 123\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['chat_history', 'current_prompt', 'expected_output'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chat_history', 'current_prompt', 'expected_output'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split csv dataset into train and temp (80% train, 20% temp)\n",
    "train_test_split = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Further split temp into validation and test (50% each → 10% of total dataset each)\n",
    "valid_test_split = train_test_split[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# Create the final DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"valid\": valid_test_split[\"train\"],\n",
    "    \"test\": valid_test_split[\"test\"]\n",
    "})\n",
    "\n",
    "# Verify the splits\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644c68d-9adf-48a4-90a2-8fd89555a302",
   "metadata": {},
   "source": [
    "### Fine-tuning the Model\n",
    "\n",
    "We will fine-tune DistilGPT2 on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39bb2e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# select optimal device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    torch.backends.mps.allow_tf32 = True\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60dd1fe-8144-4678-b018-20891e49237a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select pretrained model to fine-tune\n",
    "model_checkpoint = 'distilgpt2'\n",
    "\n",
    "# generate classification model from model_checkpoint\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# load model to device\n",
    "model.to(device)\n",
    "\n",
    "# view model architecture \n",
    "# (layers, dims, hyperparams, etc.)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f24ec8",
   "metadata": {},
   "source": [
    "### Tokenize dataset\n",
    "\n",
    "We map the tokenizer to the dataset for training. During inference, we would just tokenize that individual input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a4bcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'Lost track of time gaming again. Whoops.',\n",
       " 'current_prompt': \"Mmm, add 'but totally worth it' at the end.\",\n",
       " 'expected_output': 'Lost track of time gaming again. Whoops, but totally worth it.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe a single entry in the training dataset\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90841ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5589a9ffe374fb6a1c685098cb8cb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd8b9bbdcb94ea08afbbf1fcc6c3269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0179da6c4d844947b4d9b3e11e7129ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chat_history', 'current_prompt', 'expected_output', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 123\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['chat_history', 'current_prompt', 'expected_output', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chat_history', 'current_prompt', 'expected_output', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 16\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer for model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Apply tokenizer to the set of examples.\n",
    "\n",
    "    Merge the chat_history and current_prompt entries to form the entire prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    # merge chat_history and current_prompt\n",
    "    # we use the specialized unique token \"<|endoftext|>\" to explicitly identify \n",
    "    # indices the LLM should identify as breaks between the conversation.\n",
    "    inputs = [\n",
    "        f\"<|endoftext|> Chat history: {ch} <|endoftext|> User: {cp} <|endoftext|>\"\n",
    "        for ch, cp in zip(examples[\"chat_history\"], examples[\"current_prompt\"])\n",
    "    ]\n",
    "\n",
    "    # tokenize the resultant prompt\n",
    "    return tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=200)\n",
    "\n",
    "num_proc = 4  # use multiple cpu proc\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=num_proc)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47500035-a555-46e0-83dc-440586d96b7e",
   "metadata": {},
   "source": [
    "### Try Untrained Model on our Training Dataset\n",
    "\n",
    "Before we begin training, let's take a moment to benchmark how well the untrained model performs on our dataset. \n",
    "\n",
    "Knowing this allows us to assess whether our approach to training the model actually made a difference or not.\n",
    "\n",
    "When I (Rayyan) ran this code, the results were mainly unrelated statements about the US senate and NY Times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f3761c1-a297-45c8-882e-d74856259810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained distilgpt2 results:\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Input: <|endoftext|> Chat history: Lost track of time gaming again. Whoops <|endoftext|> User: Erase whoops <|endoftext|>\n",
      "Output:  Chat history: Lost track of time gaming again. Whoops  User: Erase whoops The following is a list of the most popular and most popular websites for the upcoming release of the new Windows Phone 8.1.1 update.\n",
      "\n",
      "\n",
      "\n",
      "The new Windows Phone 8.1 update is coming to Windows Phone 8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Input: <|endoftext|> Chat history: This movie is not worth watching even once. <|endoftext|> User: Actually, just say it sucks <|endoftext|>\n",
      "Output:  Chat history: This movie is not worth watching even once.  User: Actually, just say it sucks The first thing you need to know is how to get your hands on the latest version of the Android app.\n",
      "\n",
      "\n",
      "\n",
      "The Android version of Android is a free version of Android.\n",
      "\n",
      "\n",
      "\n",
      "The Android version of Android is a free version of Android.\n",
      "\n",
      "\n",
      "\n",
      "The Android version of Android is a free version of Android.\n",
      "\n",
      "\n",
      "The Android\n",
      "\n",
      "\n",
      "\n",
      "----------------------------\n",
      "Input: <|endoftext|> Chat history: The weather today is surprisingly nice. <|endoftext|> User: No actually, say it's serene. <|endoftext|>\n",
      "Output:  Chat history: The weather today is surprisingly nice.  User: No actually, say it's serene. The first of the three-part series of articles on the \"The New York Times\" is here.\n",
      "\n",
      "\n",
      "\n",
      "The New York Times has been a big fan of the internet for years, but the New York Times has been a little bit of a surprise.\n",
      "\n",
      "\n",
      "The New York Times has been a big fan of the internet for years, but the\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define list of examples\n",
    "text_list = [\n",
    "    \"<|endoftext|> Chat history: Lost track of time gaming again. Whoops <|endoftext|> User: Erase whoops <|endoftext|>\",\n",
    "    \"<|endoftext|> Chat history: This movie is not worth watching even once. <|endoftext|> User: Actually, just say it sucks <|endoftext|>\",\n",
    "    \"<|endoftext|> Chat history: The weather today is surprisingly nice. <|endoftext|> User: No actually, say it's serene. <|endoftext|>\"\n",
    "]\n",
    "\n",
    "print(f\"Untrained {model_checkpoint} results:\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "for text in text_list:\n",
    "    # tokenize input text and move to the correct device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # generate model predictions\n",
    "    with torch.no_grad():  # Disable gradients to save memory\n",
    "        output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "    # decode generated tokens into text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {generated_text}\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9a120-580d-470c-a981-7c7e22604865",
   "metadata": {},
   "source": [
    "### Define Training Arguments\n",
    "\n",
    "In particular, we will specify:\n",
    "- Hyperparameters\n",
    "- Training + Validation datasets\n",
    "- Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db78059-e5ae-4807-89db-b58ef6abedd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 1\n",
    "v_num = 0  # version number (if model_checkpoint_v0...vk exist, then v_num = k+1 for the next model) \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"text_edit_llm_attempts/{model_checkpoint}_v{v_num}\",  # give the resultant model a name\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # set to \"wandb\" if using Weights & Biases web app for model analysis\n",
    "    save_total_limit=2,\n",
    "    fp16=device.startswith(\"cuda\"),  # use mixed precision if on CUDA\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff356f78-c9fd-4f2b-8f5b-097cf29c1c08",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We fine-tune using the LoRA config offered by Hugging Face's PEFT (parameter-efficient fine-tuning) framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4dde538-cd7f-4ab5-a96d-c30f3003822e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'c_attn', 'c_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank\n",
    "    lora_alpha=16,  \n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 specific attention layers\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# verify config\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e0d9408-9fc4-4bd3-8d35-4d8217fe01e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 405,504 || all params: 82,318,080 || trainable%: 0.4926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# wrap model with LoRA config\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# print trainable parameters (should be much lower than full fine-tuning)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc8bc705-5dd7-4305-a797-399b2b0fa2c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      4\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m      5\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_dataset[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_dataset[\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2172\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   2173\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   2174\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   2175\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   2176\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[39m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mno_sync, model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(batch_samples) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mdistributed_type \u001b[39m!=\u001b[39m DistributedType\u001b[39m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[39melse\u001b[39;00m contextlib\u001b[39m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2533\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[39m=\u001b[39m tr_loss \u001b[39m+\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs, num_items_in_batch\u001b[39m=\u001b[39;49mnum_items_in_batch)\n\u001b[1;32m   3677\u001b[0m \u001b[39mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtorch_empty_cache_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/trainer.py:3752\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3750\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3751\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m outputs:\n\u001b[0;32m-> 3752\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3753\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3754\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(outputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. For reference, the inputs it received are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3755\u001b[0m         )\n\u001b[1;32m   3756\u001b[0m     \u001b[39m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   3757\u001b[0m     loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"valid\"],\n",
    ")\n",
    "\n",
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5664d1-9bd2-4ce1-bc24-cab5adf80f49",
   "metadata": {},
   "source": [
    "### Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc029e-1c16-491d-a3f1-715f9e0adf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of examples\n",
    "text_list = [\n",
    "    \"<|endoftext|> Chat history: Lost track of time gaming again. Whoops <|endoftext|> User: Erase whoops <|endoftext|>\",\n",
    "    \"<|endoftext|> Chat history: This movie is not worth watching even once. <|endoftext|> User: Actually, just say it sucks <|endoftext|>\",\n",
    "    \"<|endoftext|> Chat history: The weather today is surprisingly nice. <|endoftext|> User: No actually, say it's serene. <|endoftext|>\"\n",
    "]\n",
    "\n",
    "print(f\"Trained {model_checkpoint}_v{v_num} results:\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "for text in text_list:\n",
    "    # tokenize input text and move to the correct device\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # generate model predictions\n",
    "    with torch.no_grad():  # Disable gradients to save memory\n",
    "        output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "    # decode generated tokens into text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Output: {generated_text}\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dd0df",
   "metadata": {},
   "source": [
    "### Save the model locally\n",
    "\n",
    "Save the model and its tokenizer to the respectively named folders, both with identical names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bcd347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name to be saved with\n",
    "model_name = model_checkpoint + \"_v\" + {v_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ced65a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizers/lora-text-classification_0/tokenizer_config.json',\n",
       " 'tokenizers/lora-text-classification_0/special_tokens_map.json',\n",
       " 'tokenizers/lora-text-classification_0/vocab.txt',\n",
       " 'tokenizers/lora-text-classification_0/added_tokens.json',\n",
       " 'tokenizers/lora-text-classification_0/tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"models/\" + model_name)\n",
    "tokenizer.save_pretrained(\"tokenizers/\" + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c084bd9e-f7b1-4979-b753-73335ee0cede",
   "metadata": {},
   "source": [
    "### Optional: push model to hub\n",
    "\n",
    "doesnt work for me yet idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "159eb49a-dd0d-4c9e-b9ab-27e06585fd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d8d2d65d8f41f28b9ddca90e29c92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# option 1: notebook login\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # ensure token gives write access\n",
    "\n",
    "# # option 2: key login\n",
    "# from huggingface_hub import login\n",
    "# write_key = 'hf_' # paste token here\n",
    "# login(write_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09496307-e253-47e3-a46f-3f28a84c89a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_name = 'rayyanaamir' # your hf username or org name\n",
    "model_id = hf_name + \"/\" + model_checkpoint + \"-\" + model_name # you can name the model whatever you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c56ea581-0ea3-45f3-af21-362e9093ee37",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67b28545-2310f30c04b03cea101696b6;df00ea28-0fe8-4776-86bc-f14b08139618)\n\nInvalid username or password.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    407\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mpush_to_hub(model_id) \u001b[39m# save model\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/hub.py:933\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[0;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    930\u001b[0m repo_url \u001b[39m=\u001b[39m deprecated_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mrepo_url\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    931\u001b[0m organization \u001b[39m=\u001b[39m deprecated_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39morganization\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 933\u001b[0m repo_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_repo(\n\u001b[1;32m    934\u001b[0m     repo_id, private\u001b[39m=\u001b[39;49mprivate, token\u001b[39m=\u001b[39;49mtoken, repo_url\u001b[39m=\u001b[39;49mrepo_url, organization\u001b[39m=\u001b[39;49morganization\n\u001b[1;32m    935\u001b[0m )\n\u001b[1;32m    937\u001b[0m \u001b[39m# Create a new empty model card and eventually tag it\u001b[39;00m\n\u001b[1;32m    938\u001b[0m model_card \u001b[39m=\u001b[39m create_and_tag_model_card(\n\u001b[1;32m    939\u001b[0m     repo_id, tags, token\u001b[39m=\u001b[39mtoken, ignore_metadata_errors\u001b[39m=\u001b[39mignore_metadata_errors\n\u001b[1;32m    940\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/utils/hub.py:740\u001b[0m, in \u001b[0;36mPushToHubMixin._create_repo\u001b[0;34m(self, repo_id, private, token, repo_url, organization)\u001b[0m\n\u001b[1;32m    737\u001b[0m             repo_id \u001b[39m=\u001b[39m repo_id\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    738\u001b[0m         repo_id \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00morganization\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 740\u001b[0m url \u001b[39m=\u001b[39m create_repo(repo_id\u001b[39m=\u001b[39;49mrepo_id, token\u001b[39m=\u001b[39;49mtoken, private\u001b[39m=\u001b[39;49mprivate, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    741\u001b[0m \u001b[39mreturn\u001b[39;00m url\u001b[39m.\u001b[39mrepo_id\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3525\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3522\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3524\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3525\u001b[0m     hf_raise_for_status(r)\n\u001b[1;32m   3526\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   3527\u001b[0m     \u001b[39mif\u001b[39;00m exist_ok \u001b[39mand\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m409\u001b[39m:\n\u001b[1;32m   3528\u001b[0m         \u001b[39m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:477\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[39mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m \u001b[39mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[39mstr\u001b[39m(e), response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-67b28545-2310f30c04b03cea101696b6;df00ea28-0fe8-4776-86bc-f14b08139618)\n\nInvalid username or password."
     ]
    }
   ],
   "source": [
    "model.push_to_hub(model_id) # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487331a-8552-4fb2-867f-985b8fe1d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(model_id) # save trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e7feaa-b70e-4b1d-a118-23c616d14639",
   "metadata": {},
   "source": [
    "### Optional: load peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cffa01-25a4-4c86-a7fa-a84353b8caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to load peft model from hub for inference\n",
    "config = PeftConfig.from_pretrained(model_id)\n",
    "inference_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.base_model_name_or_path, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(inference_model, model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
